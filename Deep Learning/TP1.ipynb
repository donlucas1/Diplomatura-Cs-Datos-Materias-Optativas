{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8341469",
   "metadata": {},
   "source": [
    "# Aprendizaje Profundo: Práctico \n",
    "\n",
    "En este práctico trabajaremos en el problema de clasificación de texto del MeLi Challenge 2019.\n",
    "\n",
    "<img src=\"images/img1_practico.png\"\n",
    "     alt=\"MeLi Challenge Spanish\"\n",
    "     style=\"float: center; margin-right: 10px;\"\n",
    "     width=80%/>\n",
    "\n",
    "El datasets tiene información acerca de títulos de publicaciones, categoría de los mismos, información de idioma y confiabilidad de la anotación.\n",
    "Cuenta con anotaciones de títulos para 632 categorías distintas.\n",
    "\n",
    "<img src=\"images/img2_practico.png\"\n",
    "     alt=\"Categorias\"\n",
    "     style=\"float: center; margin-right: 10px;\"\n",
    "     width=50%/>\n",
    "\n",
    "El dataset también cuenta con una partición de test que está compuesta de 63680 de ejemplos con las mismas categorías\n",
    "(aunque no necesariamente la misma distribución).\n",
    "\n",
    "También hay datos en idioma portugues, aunque para el práctico de esta materia basta con usar uno solo de los idiomas.\n",
    "\n",
    "<img src=\"images/img3_practico.png\"\n",
    "     alt=\"MeLi Challenge Portuguese\"\n",
    "     style=\"float: center; margin-right: 10px;\"\n",
    "     width=80%/>\n",
    "\n",
    "## Ejercicio:\n",
    "Implementar una red neuronal que asigne una categoría dado un título.\n",
    "Para este práctico se puede usar cualquier tipo de red neuronal. Les que hagan solo la primera mitad de la materia,\n",
    "implementarán un MLP. Quienes cursan la materia completa, deberían implementar algo más complejo, usando CNNs,\n",
    "RNNs o Transformers.\n",
    "\n",
    "<img src=\"images/img4_practico.png\"\n",
    "     alt=\"NN Generic Architecture\"\n",
    "     style=\"float: center; margin-right: 10px;\"\n",
    "     width=40%/>\n",
    "\n",
    "\n",
    "Algunas consideraciones a tener en cuenta para estructurar el trabajo:\n",
    "\n",
    "  1. Hacer un preprocesamiento de los datos (¿Cómo vamos a representar los datos de entrada y las categorías?).\n",
    "  2. Tener un manejador del dataset (alguna clase o función que nos divida los datos en batches).\n",
    "  3. Crear una clase para el modelo que se pueda instanciar con diferentes hiperparámetros\n",
    "  4. Hacer logs de entrenamiento (reportar tiempo transcurrido, iteraciones/s, loss, accuracy, etc.). Usar MLFlow.\n",
    "  5. Hacer un gráfico de la función de loss a lo largo de las epochs. MLFlow también puede generar la gráfica.\n",
    "  6. Reportar performance en el conjunto de test con el mejor modelo entrenado. La métrica para reportar será balanced accuracy ([Macro-recall](https://peltarion.com/knowledge-center/documentation/evaluation-view/classification-loss-metrics/macro-recall)).\n",
    "\n",
    "## Ejercicios opcionales:\n",
    "  1. Se puede tomar una subconjunto del train para armar un set de validation para probar configuraciones, arquitecturas o hiperparámetros (cuidado con la distribución de este conjunto). **No usar el conjunto de test para ajustar hiperparámetros.**\n",
    "  2. Se puede usar todos los datos (spanish & portuguese) para hacer un modelo multilingual para la tarea. ¿Qué cosas tendrían que tener en cuenta para ello?\n",
    "\n",
    "Adicionalmente, se pide un reporte de los experimentos y los procesos que se llevaron a cabo (en el README.md de su repositorio correspondiente). \n",
    "No se evaluará la performance de los modelos, sino el proceso de tomar el problema e implementar una solución con aprendizaje profundo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a8fe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4378d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json(f\"./data/meli-challenge-2019/spanish.train.jsonl.gz\", lines=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809e107",
   "metadata": {},
   "source": [
    "\n",
    "df_train.to_csv (r'./data/meli-challenge-2019/meli_train.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1e8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.read_json(f\"./data/meli-challenge-2019/spanish.validation.jsonl.gz\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07e50e",
   "metadata": {},
   "source": [
    "df_validation.to_csv (r'./data/meli-challenge-2019/meli_validation.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4245de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test =  pd.read_json(f\"./data/meli-challenge-2019/spanish.test.jsonl.gz\", lines=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d86b4",
   "metadata": {},
   "source": [
    "df_test.to_csv (r'./data/meli-challenge-2019/meli_test.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ece0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = pd.read_json(f\"./data/meli-challenge-2019/spanish_token_to_index.json.gz\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6ee4fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barbies</th>\n",
       "      <th>casita</th>\n",
       "      <th>muñecas</th>\n",
       "      <th>pintadas</th>\n",
       "      <th>cromado</th>\n",
       "      <th>holográfico</th>\n",
       "      <th>neceser</th>\n",
       "      <th>asiento</th>\n",
       "      <th>chevrolet</th>\n",
       "      <th>funda</th>\n",
       "      <th>...</th>\n",
       "      <th>chorros</th>\n",
       "      <th>budadá</th>\n",
       "      <th>cáp</th>\n",
       "      <th>sebium</th>\n",
       "      <th>hlertfx</th>\n",
       "      <th>promix</th>\n",
       "      <th>acrilab</th>\n",
       "      <th>galapagos</th>\n",
       "      <th>[PAD]</th>\n",
       "      <th>[UNK]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50000</td>\n",
       "      <td>50001</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>49992</td>\n",
       "      <td>49993</td>\n",
       "      <td>49994</td>\n",
       "      <td>49995</td>\n",
       "      <td>49996</td>\n",
       "      <td>49997</td>\n",
       "      <td>49998</td>\n",
       "      <td>49999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   barbies  casita  muñecas  pintadas  cromado  holográfico  neceser  asiento  \\\n",
       "0    50000   50001        2         3        4            5        6        7   \n",
       "\n",
       "   chevrolet  funda  ...  chorros  budadá    cáp  sebium  hlertfx  promix  \\\n",
       "0          8      9  ...    49992   49993  49994   49995    49996   49997   \n",
       "\n",
       "   acrilab  galapagos  [PAD]  [UNK]  \n",
       "0    49998      49999      0      1  \n",
       "\n",
       "[1 rows x 50002 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faccb082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>n_labels</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Mochilas Maternales Bolsos Bebe Simil Cuero Ma...</td>\n",
       "      <td>DIAPER_BAGS</td>\n",
       "      <td>test</td>\n",
       "      <td>[mochilas, maternales, bolsos, bebe, simil, cu...</td>\n",
       "      <td>[5650, 5271, 5268, 915, 2724, 375, 37363]</td>\n",
       "      <td>318</td>\n",
       "      <td>632</td>\n",
       "      <td>63680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Bolso Maternal/bebe Incluye Cambiador + Correa...</td>\n",
       "      <td>DIAPER_BAGS</td>\n",
       "      <td>test</td>\n",
       "      <td>[bolso, maternal, bebe, incluye, cambiador, co...</td>\n",
       "      <td>[502, 2742, 915, 3031, 2740, 1840, 4635]</td>\n",
       "      <td>318</td>\n",
       "      <td>632</td>\n",
       "      <td>63680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Mochila Maternal Land  + Gancho Envio Gratis-cc</td>\n",
       "      <td>DIAPER_BAGS</td>\n",
       "      <td>test</td>\n",
       "      <td>[mochila, maternal, land, gancho, envio, gratis]</td>\n",
       "      <td>[337, 2742, 2741, 3303, 211, 1429]</td>\n",
       "      <td>318</td>\n",
       "      <td>632</td>\n",
       "      <td>63680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Bolso Maternal Moderno Con Cambiador Y Correa ...</td>\n",
       "      <td>DIAPER_BAGS</td>\n",
       "      <td>test</td>\n",
       "      <td>[bolso, maternal, moderno, cambiador, correa, ...</td>\n",
       "      <td>[502, 2742, 2983, 2740, 1840, 476, 2990]</td>\n",
       "      <td>318</td>\n",
       "      <td>632</td>\n",
       "      <td>63680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Bolso Maternal Moderno Con Cambiador Y Correa ...</td>\n",
       "      <td>DIAPER_BAGS</td>\n",
       "      <td>test</td>\n",
       "      <td>[bolso, maternal, moderno, cambiador, correa, ...</td>\n",
       "      <td>[502, 2742, 2983, 2740, 1840, 4635]</td>\n",
       "      <td>318</td>\n",
       "      <td>632</td>\n",
       "      <td>63680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language label_quality                                              title  \\\n",
       "0  spanish      reliable  Mochilas Maternales Bolsos Bebe Simil Cuero Ma...   \n",
       "1  spanish      reliable  Bolso Maternal/bebe Incluye Cambiador + Correa...   \n",
       "2  spanish      reliable    Mochila Maternal Land  + Gancho Envio Gratis-cc   \n",
       "3  spanish      reliable  Bolso Maternal Moderno Con Cambiador Y Correa ...   \n",
       "4  spanish      reliable  Bolso Maternal Moderno Con Cambiador Y Correa ...   \n",
       "\n",
       "      category split                                    tokenized_title  \\\n",
       "0  DIAPER_BAGS  test  [mochilas, maternales, bolsos, bebe, simil, cu...   \n",
       "1  DIAPER_BAGS  test  [bolso, maternal, bebe, incluye, cambiador, co...   \n",
       "2  DIAPER_BAGS  test   [mochila, maternal, land, gancho, envio, gratis]   \n",
       "3  DIAPER_BAGS  test  [bolso, maternal, moderno, cambiador, correa, ...   \n",
       "4  DIAPER_BAGS  test  [bolso, maternal, moderno, cambiador, correa, ...   \n",
       "\n",
       "                                        data  target  n_labels   size  \n",
       "0  [5650, 5271, 5268, 915, 2724, 375, 37363]     318       632  63680  \n",
       "1   [502, 2742, 915, 3031, 2740, 1840, 4635]     318       632  63680  \n",
       "2         [337, 2742, 2741, 3303, 211, 1429]     318       632  63680  \n",
       "3   [502, 2742, 2983, 2740, 1840, 476, 2990]     318       632  63680  \n",
       "4        [502, 2742, 2983, 2740, 1840, 4635]     318       632  63680  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2f287",
   "metadata": {},
   "source": [
    "Las bases de datos ya estan tokenizadas. Vemos que en la columna data nos aparece el nro correspondiente a sus respectivos titulos tokenizados. la base de datos token to index, nos remite directamente nro por nombre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "548600d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainR = df_train.drop(columns=['language', 'label_quality', 'title',\n",
    "                               'category', 'split', 'tokenized_title',\n",
    "                               'n_labels', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcc1de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ValiR = df_validation.drop(columns=['language', 'label_quality', 'title',\n",
    "                               'category', 'split', 'tokenized_title',\n",
    "                               'n_labels', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c7b8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testR = df_test.drop(columns=['language', 'label_quality', 'title',\n",
    "                               'category', 'split', 'tokenized_title',\n",
    "                               'n_labels', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "300fdc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[50001, 2, 50000, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[9, 7, 10, 8]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 13, 12, 14]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[15, 19, 17, 18, 16, 1, 1, 1]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            data  target\n",
       "0           [50001, 2, 50000, 3]       0\n",
       "1                      [6, 4, 5]       1\n",
       "2                  [9, 7, 10, 8]       2\n",
       "3               [11, 13, 12, 14]       3\n",
       "4  [15, 19, 17, 18, 16, 1, 1, 1]       4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trainR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c8e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.target\n",
    "X = df_train.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a89083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            0\n",
       "1            1\n",
       "2            2\n",
       "3            3\n",
       "4            4\n",
       "          ... \n",
       "4895275     28\n",
       "4895276     24\n",
       "4895277     74\n",
       "4895278     74\n",
       "4895279    388\n",
       "Name: target, Length: 4895280, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ae448aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de etiquetas: 26129139\n",
      "Cantidad de etiquetas diferentes: 50001\n",
      "Valor máximo: 50001\n",
      "Valor mínimo: 1\n"
     ]
    }
   ],
   "source": [
    "data = df_trainR.data\n",
    "\n",
    "lista_num_palabras = []\n",
    "for id_palabras in data:\n",
    "    lista_num_palabras += id_palabras\n",
    "    \n",
    "\n",
    "print('Cantidad de etiquetas:', len(lista_num_palabras))\n",
    "print('Cantidad de etiquetas diferentes:', len(set(lista_num_palabras)))\n",
    "print('Valor máximo:', max(lista_num_palabras))\n",
    "print('Valor mínimo:', min(lista_num_palabras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "357f640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632 etiquetas ordenadas como se muestra a continuación:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "       624, 625, 626, 627, 628, 629, 630, 631], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = df_trainR.target.unique()\n",
    "print(f'{labels.shape[0]} etiquetas ordenadas como se muestra a continuación:')\n",
    "np.sort(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b138cc",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7777a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fd71528",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5837bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5e42e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6c0b52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 ignore_header=True, \n",
    "                 filters=None, \n",
    "                 vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "        \n",
    "        # Create dictionary based on all the reviews (with corresponding preprocessing)\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"data\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        # Filter the dictionary with extremos words\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html?highlight=filter_extrem\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        \n",
    "        # Make the indices continuous after some words have been removed\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.compactify.html\n",
    "        self.dictionary.compactify()\n",
    "        \n",
    "        # Add a couple of special tokens\n",
    "        self.dictionary.patch_with_special_tokens({\n",
    "            \"[PAD]\": 0,\n",
    "            \"[UNK]\": 1\n",
    "        })\n",
    "        self.idx_to_target = sorted(dataset[\"category\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        # https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string:~:text=gensim.parsing.preprocessing.preprocess_string\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "      # https://radimrehurek.com/gensim/corpora/dictionary.html#:~:text=doc2idx(document,via%20unknown_word_index.\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ee9f2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "        \n",
    "        item = {\n",
    "            \"data\": self.dataset.loc[item, \"title\"],\n",
    "            \"target\": self.dataset.loc[item, \"category\"]\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4dae352",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mRawDataProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m MyDataset(df_train, transform\u001b[38;5;241m=\u001b[39mpreprocess)\n\u001b[0;32m      3\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m RawDataProcessor(df_validation)\n",
      "Input \u001b[1;32mIn [108]\u001b[0m, in \u001b[0;36mRawDataProcessor.__init__\u001b[1;34m(self, dataset, ignore_header, filters, vocab_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m     12\u001b[0m         preprocessing\u001b[38;5;241m.\u001b[39mstrip_tags,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m         preprocessing\u001b[38;5;241m.\u001b[39mstrip_short,\n\u001b[0;32m     18\u001b[0m     ]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create dictionary based on all the reviews (with corresponding preprocessing)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# https://radimrehurek.com/gensim/corpora/dictionary.html\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_string\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Filter the dictionary with extremos words\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html?highlight=filter_extrem\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mfilter_extremes(no_below\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, no_above\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keep_n\u001b[38;5;241m=\u001b[39mvocab_size)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4237\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg, na_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   4163\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4235\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   4236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4239\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4240\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py:880\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# mapper is a function\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mmap_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_values\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [108]\u001b[0m, in \u001b[0;36mRawDataProcessor._preprocess_string\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preprocess_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, string):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string:~:text=gensim.parsing.preprocessing.preprocess_string\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\parsing\\preprocessing.py:442\u001b[0m, in \u001b[0;36mpreprocess_string\u001b[1;34m(s, filters)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_string\u001b[39m(s, filters\u001b[38;5;241m=\u001b[39mDEFAULT_FILTERS):\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply list of chosen filters to `s`.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03m    Default list of filters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filters:\n\u001b[0;32m    444\u001b[0m         s \u001b[38;5;241m=\u001b[39m f(s)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py:365\u001b[0m, in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "preprocess = RawDataProcessor(df_train)\n",
    "train_dataset = MyDataset(df_train, transform=preprocess)\n",
    "preprocess = RawDataProcessor(df_validation)\n",
    "\n",
    "test_dataset = MyDataset(df_validation, transform=preprocess)\n",
    "\n",
    "print(f\"Datasets loaded with {len(train_dataset)} training elements and {len(test_dataset)} test elements\")\n",
    "print(f\"Sample train element:\\n{train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b117a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c705c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = PadSequences()\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                          collate_fn=pad_sequences, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,\n",
    "                         collate_fn=pad_sequences, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b5f08657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(650, 1)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7cad647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = nn.Linear(32 * 32 * 3, 512)\n",
    "        self.hidden_layer2 = nn.Linear(512, 256)\n",
    "        self.output_layer = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.hidden_layer1(x)  # Go through hidden layer 1\n",
    "        x = F.relu(x)  # Activation Function layer 1\n",
    "        x = self.hidden_layer2(x) # Go through hidden layer 2\n",
    "        x = F.relu(x)  # Activation Function layer 2\n",
    "        x = self.output_layer(x)  # Output Layer\n",
    "        return x\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e928af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden_layer1): Linear(in_features=3072, out_features=512, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (output_layer): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba8bf5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "883f8dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyDataset at 0x2606d7b1fd0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ad43d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f69b8f2bfe4c7398730e89a373c20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4895280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "model.train()  # Tell the model to set itself to \"train\" mode.\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_dataset)\n",
    "    for i, data in enumerate(pbar, 1):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.view(inputs.shape[0], -1))\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i > 0 and i % 50 == 0:    # print every 50 mini-batches\n",
    "            pbar.set_description(f\"[{epoch + 1}, {i}] loss: {running_loss / 50:.4g}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3be68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc3c60ae",
   "metadata": {},
   "source": [
    "# CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "325d9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "        \n",
    "        item = {\n",
    "            \"data\": self.dataset.loc[item, \"data\"],\n",
    "            \"target\": self.dataset.loc[item, \"target\"]\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d55345e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 ignore_header=True, \n",
    "                 filters=None, \n",
    "                 vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "        \n",
    "        # Create dictionary based on all the reviews (with corresponding preprocessing)\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"data\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        # Filter the dictionary and compactify it (make the indices continous)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        self.dictionary.compactify()\n",
    "        # Add a couple of special tokens\n",
    "        self.dictionary.patch_with_special_tokens({\n",
    "            \"[PAD]\": 0,\n",
    "            \"[UNK]\": 1\n",
    "        })\n",
    "        self.idx_to_target = sorted(dataset[\"target\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0eb4a428",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/meli-challenge-2019/meli_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mRawDataProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_indices, test_indices \u001b[38;5;241m=\u001b[39m train_test_split(dataset\u001b[38;5;241m.\u001b[39mindex, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m IMDBReviewsDataset(dataset\u001b[38;5;241m.\u001b[39mloc[train_indices]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), transform\u001b[38;5;241m=\u001b[39mpreprocess)\n",
      "Input \u001b[1;32mIn [106]\u001b[0m, in \u001b[0;36mRawDataProcessor.__init__\u001b[1;34m(self, dataset, ignore_header, filters, vocab_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcompactify()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Add a couple of special tokens\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_with_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[UNK]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_to_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_to_idx \u001b[38;5;241m=\u001b[39m {t: i \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_to_target)}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py:633\u001b[0m, in \u001b[0;36mDictionary.patch_with_special_tokens\u001b[1;34m(self, special_token_dict)\u001b[0m\n\u001b[0;32m    631\u001b[0m     possible_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id[token])\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id[token]\n\u001b[1;32m--> 633\u001b[0m old_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id[token] \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id[old_token] \u001b[38;5;241m=\u001b[39m possible_ids\u001b[38;5;241m.\u001b[39mpop() \u001b[38;5;28;01mif\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m                            \u001b[38;5;28mlen\u001b[39m(possible_ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py:107\u001b[0m, in \u001b[0;36mDictionary.__getitem__\u001b[1;34m(self, tokenid)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2token) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# the word->id mapping has changed (presumably via add_documents);\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# recompute id->word accordingly\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2token \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mrevdict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid2token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenid\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./data/meli-challenge-2019/meli_train.csv\")\n",
    "preprocess = RawDataProcessor(dataset)\n",
    "train_indices, test_indices = train_test_split(dataset.index, test_size=0.2, random_state=42)\n",
    "train_dataset = IMDBReviewsDataset(dataset.loc[train_indices].reset_index(drop=True), transform=preprocess)\n",
    "test_dataset = IMDBReviewsDataset(dataset.loc[test_indices].reset_index(drop=True), transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140f755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c67a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed294cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
